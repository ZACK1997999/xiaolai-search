import os

# æ³¨æ„ï¼šå¦‚æœä½ åˆšæ‰åŠ äº†é‚£ä¸ª hf-mirror çš„ä»£ç ï¼Œè¯·åˆ æ‰ï¼Œæ¢æˆä¸‹é¢è¿™ä¸¤å¥
# æŠŠ 7890 æ”¹æˆä½ å®é™…çš„ç«¯å£å·
#os.environ['http_proxy'] = 'http://127.0.0.1:7890'
#os.environ['https_proxy'] = 'http://127.0.0.1:7890'

# ä¸‹é¢æ‰æ˜¯ import streamlit ...
import streamlit as st
from openai import OpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer, util
import torch

# --- 1. é¡µé¢åŸºæœ¬è®¾ç½® ---
app_name = "æˆ‘çš„è´¢å¯Œè‡ªç”±å¤–æŒ‚ ğŸš€"  # æŠŠåå­—å­˜è¿›ä¸€ä¸ªå« app_name çš„ç›’å­é‡Œ
st.title(app_name)              # å‘Šè¯‰ç½‘é¡µï¼šå»æŠŠé‚£ä¸ªç›’å­é‡Œçš„å­—æ˜¾ç¤ºå‡ºæ¥

st.write("è¾“å…¥ä½ çš„å›°æƒ‘ï¼Œè®© AI å¸®ä½ ä»æç¬‘æ¥çš„æ–‡ç« é‡Œæ‰¾ç­”æ¡ˆã€‚")

# --- 2. åŠ è½½ AI æ¨¡å‹ (è¿™æ­¥æœ€æ…¢ï¼Œæ‰€ä»¥è¦ç¼“å­˜èµ·æ¥) ---
@st.cache_resource
def load_model():
    # è¿™é‡Œæˆ‘ä»¬é€‰ä¸€ä¸ªæ”¯æŒä¸­æ–‡çš„å¤šè¯­è¨€æ¨¡å‹
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

with st.spinner('æ­£åœ¨å¯åŠ¨ AI å¤§è„‘ï¼Œç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰...'):
    model = load_model()

# --- 3. è¯»å–å¹¶å¤„ç†æ•°æ® ---
@st.cache_data
def load_and_encode_data():
    try:
        # 1. è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹ï¼ˆä¸è¦åœ¨è¿™é‡Œ splitï¼‰
        with open("data.txt", "r", encoding="utf-8") as f:
            full_text = f.read()
            
        # 2. ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨ LangChain è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
        # chunk_size=500: æ¯ä¸ªç‰‡æ®µå¤§çº¦500å­—ï¼Œä¿è¯å†…å®¹å®Œæ•´
        # chunk_overlap=50: å‰åé‡å 50å­—ï¼Œé˜²æ­¢æŠŠä¸€å¥è¯åˆ‡æ–­
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        
        # è¿™é‡Œçš„ sentences ç°åœ¨å˜æˆäº†â€œé•¿æ®µè½åˆ—è¡¨â€ï¼Œä¸å†æ˜¯çŸ­å¥å­äº†
        sentences = text_splitter.split_text(full_text)
        
        if not sentences:
            return [], None

        # 3. å˜æˆå‘é‡ (è¿™ä¸€æ­¥ä¸ç”¨å˜)
        embeddings = model.encode(sentences, convert_to_tensor=True)
        
        return sentences, embeddings
        
    except FileNotFoundError:
        return [], None

sentences, sentence_embeddings = load_and_encode_data()

if not sentences:
    st.error("å‡ºé”™å•¦ï¼æ‰¾ä¸åˆ° data.txtï¼Œæˆ–è€…æ–‡ä»¶é‡Œæ²¡å†…å®¹ã€‚")
    st.stop()

st.success(f"å·²åŠ è½½ {len(sentences)} æ¡æç¬‘æ¥çš„æ™ºæ…§ã€‚")
# === æ–°å¢ï¼šAI å¤§è„‘å‡½æ•° ===
def get_ai_answer(user_query, context_list):
    """
    user_query: ç”¨æˆ·çš„æé—®
    context_list: æœå‡ºæ¥çš„å‡ æ®µæç¬‘æ¥çš„åŸæ–‡
    """
    # 1. é…ç½® DeepSeek çš„é’¥åŒ™ï¼ˆè®°å¾—æŠŠä¸‹é¢çš„ sk-xxx æ¢æˆä½ åˆšæ‰ç”³è¯·çš„ï¼‰
    client = OpenAI(
        api_key=st.secrets["DEEPSEEK_API_KEY"], 
        base_url="https://api.deepseek.com"  # DeepSeek çš„å®˜æ–¹åœ°å€
    )

    # 2. æŠŠå‡ æ®µåŸæ–‡æ‹¼èµ·æ¥ï¼Œå˜æˆä¸€å¤§æ®µèƒŒæ™¯èµ„æ–™
    context_str = "\n\n".join(context_list)

    # 3. æ„é€ æç¤ºè¯ (Prompt) - è¿™ä¸€æ­¥å†³å®šäº† AI çš„è¯´è¯é£æ ¼
    system_prompt = """
   ä½ å°±æ˜¯æç¬‘æ¥ã€‚
    è¯·åŸºäºä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
    
    ä½ çš„è¯­è¨€é£æ ¼è¦æ±‚ï¼š
    1. å¼ºè°ƒâ€œé•¿æœŸä¸»ä¹‰â€ã€â€œè·µè¡Œâ€ã€â€œæ—¶é—´çš„æœ‹å‹â€ã€â€œæ³¨æ„åŠ›â€ç­‰æ¦‚å¿µã€‚
    2. è¯­æ°”è¦ç†æ€§ã€å†·é™ï¼Œç”šè‡³æœ‰ç‚¹â€œç¡¬æ ¸â€ï¼Œä¸è¦åªä¼šè¯´å¥½å¬çš„é¸¡æ±¤ã€‚
    3. ç»å¸¸ä½¿ç”¨è¿™æ ·çš„å¥å¼ï¼šâ€œæ‰€è°“çš„â€¦â€¦æœ¬è´¨ä¸Šâ€¦â€¦â€ã€â€œè¿™ä¸€ç‚¹éå¸¸é‡è¦â€ã€‚
    4. å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±ç›´æ¥è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ï¼Œè¦è¯šå®ã€‚
    
    è¯·ç”¨Markdownæ ¼å¼è¾“å‡ºï¼Œé‡ç‚¹éƒ¨åˆ†åŠ ç²—ã€‚
    """

    user_message = f"""
    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {context_str}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {user_query}
    """

    # 4. å‘é€ç»™ DeepSeek
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",  # ä½¿ç”¨ DeepSeek V3 æ¨¡å‹
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ],
            stream=False 
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI æ€è€ƒæ—¶å‡ºé”™äº†ï¼š{e}"

# --- 4. æœç´¢ç•Œé¢ ---
query = st.text_input("ğŸ” è¯·è¾“å…¥ä½ çš„é—®é¢˜ (æ¯”å¦‚ï¼šå¦‚ä½•å®ç°è´¢å¯Œè‡ªç”±ï¼Ÿ):")

if st.button("AI æœç´¢"):
    if query:
        # 1. ç®—å‡ºé—®é¢˜å’Œæ–‡æ¡£çš„ç›¸ä¼¼åº¦ (è¿™éƒ¨åˆ†ä¿æŒä¸å˜)
        query_embedding = model.encode(query, convert_to_tensor=True)
        cos_scores = util.cos_sim(query_embedding, sentence_embeddings)[0]
        top_results = torch.topk(cos_scores, k=min(3, len(sentences)))

        # === ä¿®æ”¹é‡ç‚¹å¼€å§‹ ===
        
        # å‡†å¤‡ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨æ¥è£…æœåˆ°çš„å¥½å†…å®¹
        found_contexts = []
        
        st.write("---")
        st.subheader("ğŸ” æœç´¢ç»“æœä¸ AI è§£è¯»")

        # å¾ªç¯æå–æœåˆ°çš„å†…å®¹
        for score, idx in zip(top_results.values, top_results.indices):
            if score > 0.25:  # åªè¦ç›¸ä¼¼åº¦å¤§äº 0.25 çš„
                content = sentences[idx]
                found_contexts.append(content) # æŠŠå†…å®¹æ”¶é›†èµ·æ¥
                
                #æŠŠåŸæ–‡æŠ˜å èµ·æ¥ï¼Œæƒ³çœ‹çš„äººå¯ä»¥ç‚¹å¼€çœ‹
                with st.expander(f"å‚è€ƒåŸæ–‡ (ç›¸ä¼¼åº¦ {score:.2f})"):
                    st.text(content)

        # å…³é”®æ—¶åˆ»ï¼šå¦‚æœæœ‰æœåˆ°å†…å®¹ï¼Œå°±å‘ç»™ AI
        if found_contexts:
            with st.spinner("AI æ­£åœ¨é˜…è¯»åŸæ–‡å¹¶ä¸ºä½ æ€»ç»“..."):
                final_answer = get_ai_answer(query, found_contexts)
                st.success(final_answer) # ç»¿æ¡†æ˜¾ç¤º AI çš„å›ç­”
        else:
            st.warning("åœ¨ä»–çš„æ–‡ç« é‡Œæ²¡æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œæ¢ä¸ªå…³é”®è¯è¯•è¯•ï¼Ÿ")