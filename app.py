import streamlit as st
import pandas as pd
from openai import OpenAI
import json
import re

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ”´ è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ çš„çœŸå® Keyï¼Œä½ çš„æœ‹å‹åœ¨ç½‘é¡µä¸Šæ˜¯çœ‹ä¸åˆ°è¿™ä¸ªçš„
# æ³¨æ„ï¼šä¸è¦æŠŠè¿™ä¸ªæ–‡ä»¶å‘ç»™é™Œç”Ÿäººï¼Œå¦åˆ™ä»–ä»¬èƒ½çœ‹åˆ°ä½ çš„ Key
# ================= é…ç½®åŒºåŸŸ =================
try:
    # å°è¯•ä» Streamlit çš„äº‘ç«¯â€œä¿é™©ç®±â€è·å– Key
    MY_HIDDEN_KEY = st.secrets["DEEPSEEK_API_KEY"]
except FileNotFoundError:
    st.error("æœªæ‰¾åˆ°å¯†é’¥ï¼è¯·é…ç½® .streamlit/secrets.toml æˆ–åœ¨äº‘ç«¯è®¾ç½® Secretsã€‚")
    st.stop()
# ===========================================
# ===========================================

# --- å®‰å…¨åŠ è½½ YouTube æ¨¡å— ---
YOUTUBE_AVAILABLE = False
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    if not hasattr(YouTubeTranscriptApi, 'get_transcript'):
        raise ImportError
    YOUTUBE_AVAILABLE = True
except:
    YOUTUBE_AVAILABLE = False

# ================= ç¬¬ä¸€éƒ¨åˆ†ï¼šPreply é£æ ¼ç²¾å‡†æµ‹è¯• =================

def run_vocab_test():
    st.header("ğŸ“ˆ è‹±è¯­è¯æ±‡é‡è¯„ä¼°")
    st.caption("ä¸ºäº†è®© AI åŠ©æ•™æ›´æ‡‚ä½ ï¼Œè¯·å…ˆå®Œæˆä¸¤æ­¥å¿«é€Ÿæµ‹è¯•ã€‚")
    st.info("è¯·åªå‹¾é€‰ä½ **ç¡®å®è®¤è¯†**ï¼ˆèƒ½è¯´å‡ºä¸­æ–‡æ„æ€ï¼‰çš„å•è¯ã€‚")
    
    step1_words = [
        "red", "bus", "salt", "rabbit", "hammer", 
        "sudden", "barely", "attend", "defend", "modest",
        "justice", "specialize", "harvest", "threshold", "mechanic",
        "ambiguous", "magnitude", "reinforce", "profound", "allegation",
        "manifestation", "conspiracy", "indigenous", "hypothesis", "pragmatic",
        "ubiquitous", "ephemeral", "meticulous", "exacerbate", "scrutinize",
        "esoteric", "vicarious", "obsequious", "idiosyncrasy", "sycophant"
    ]
    
    if 'test_stage' not in st.session_state:
        st.session_state['test_stage'] = 1
    
    # --- é˜¶æ®µ 1 ---
    if st.session_state['test_stage'] == 1:
        st.subheader("ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿå®šä½")
        cols = st.columns(5)
        selected_step1 = []
        for i, word in enumerate(step1_words):
            with cols[i % 5]:
                if st.checkbox(word, key=f"s1_{word}"):
                    selected_step1.append(word)
        
        st.write("---")
        if st.button("ç»§ç»­ä¸‹ä¸€æ­¥", type="primary"):
            # ç®€å•å®šçº§é€»è¾‘
            if len(selected_step1) < 10: st.session_state['temp_level'] = 'basic'
            elif len(selected_step1) < 20: st.session_state['temp_level'] = 'intermediate'
            else: st.session_state['temp_level'] = 'advanced'
            
            st.session_state['test_stage'] = 2
            st.rerun()

    # --- é˜¶æ®µ 2 ---
    elif st.session_state['test_stage'] == 2:
        st.subheader("ç¬¬äºŒæ­¥ï¼šç²¾å‡†æ ¡å‡†")
        level = st.session_state.get('temp_level', 'intermediate')
        
        if level == 'basic':
            step2_words = ["cousin", "leather", "shelf", "pure", "shout", "dust", "belief", "pale", "wander", "squeeze", "curious", "bunch", "terror", "faint", "weed"]
            base_score = 1000; multiplier = 150
        elif level == 'intermediate':
            step2_words = ["subtle", "distinct", "prohibit", "adequate", "consult", "guarantee", "confront", "precious", "resign", "inherit", "scatter", "courage", "bloom", "polish", "frown"]
            base_score = 4000; multiplier = 300
        else:
            step2_words = ["cynical", "eloquent", "hinder", "plausible", "tedious", "rigorous", "subsequent", "integration", "proposition", "adverse", "mitigate", "consensus", "intriguing", "viability", "fluctuation"]
            base_score = 8000; multiplier = 500
            
        st.write(f"æ­£åœ¨æ ¡å‡†... è¯·å‹¾é€‰ä½ è®¤è¯†çš„å•è¯ï¼š")
        cols2 = st.columns(5)
        selected_step2 = []
        for i, word in enumerate(step2_words):
            with cols2[i % 5]:
                if st.checkbox(word, key=f"s2_{word}"):
                    selected_step2.append(word)
        
        st.write("---")
        if st.button("ç”Ÿæˆæˆ‘çš„å­¦ä¹ æ¡£æ¡ˆ", type="primary"):
            vocab_size = base_score + (len(selected_step2) * multiplier)
            st.session_state['user_vocab_size'] = vocab_size
            
            # ç”Ÿæˆ Prompt
            if vocab_size < 3000:
                desc = "åˆå­¦è€… (è¯æ±‡é‡çº¦3000)ã€‚è¯·æå–æ‰€æœ‰éåŸºç¡€çš„ç”Ÿè¯ã€å¸¸ç”¨çŸ­è¯­ã€‚"
            elif vocab_size < 6000:
                desc = "ä¸­çº§å­¦ä¹ è€… (è¯æ±‡é‡çº¦5000ï¼Œå››çº§æ°´å¹³)ã€‚è¯·ç•¥è¿‡ç®€å•çš„è¯ï¼Œé‡ç‚¹æŒ–æ˜å››å…­çº§éš¾åº¦çš„è¯ã€åœ°é“çŸ­è¯­å’Œç†Ÿè¯ç”Ÿä¹‰ã€‚"
            elif vocab_size < 10000:
                desc = "ä¸­é«˜çº§å­¦ä¹ è€… (è¯æ±‡é‡çº¦8000ï¼Œé›…æ€/æ‰˜ç¦æ°´å¹³)ã€‚è¯·åªæå–å­¦æœ¯è¯æ±‡ã€ä¹ è¯­æ­é…ã€ä»¥åŠæ·±å±‚çš„ç†Ÿè¯ç”Ÿä¹‰ã€‚"
            else:
                desc = "é«˜é˜¶å­¦ä¹ è€… (è¯æ±‡é‡10000+)ã€‚è¯·åªæŒ–æ˜æå…¶ç½•è§çš„ç”Ÿåƒ»è¯ã€æ–‡å­¦æ€§è¯æ±‡ã€ä»¥åŠéšå–»ç”¨æ³•ã€‚"
                
            st.session_state['user_profile_prompt'] = desc
            st.rerun()

# ================= ç¬¬äºŒéƒ¨åˆ†ï¼šAI æ™ºèƒ½æŒ–æ˜ =================

class SmartMiner:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    def get_youtube_text(self, url):
        if not YOUTUBE_AVAILABLE: return "âŒ YouTube ç»„ä»¶ä¸å¯ç”¨"
        try:
            video_id = url.split("v=")[-1].split("&")[0] if "v=" in url else url.split("/")[-1].split("?")[0]
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
            return " ".join([t['text'] for t in transcript])
        except Exception as e:
            return f"Error: {e}"

    def analyze_text_with_ai(self, text, user_profile):
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­ç§æ•™ã€‚
        ã€ç”¨æˆ·ç”»åƒã€‘ï¼š{user_profile}
        
        ä½ çš„ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æŒ–æ˜é€‚åˆè¯¥ç”¨æˆ·çš„å­¦ä¹ ç´ æã€‚è¯·æå–ä»¥ä¸‹ä¸‰ç±»ï¼š
        1. **ç”Ÿè¯** (Words)ã€‚
        2. **çŸ­è¯­** (Phrases)ã€‚
        3. **ç†Ÿè¯ç”Ÿä¹‰** (Polysemy)ã€‚

        è¯·ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¿”å›åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        [
            {{
                "word": "åŸå‹",
                "type": "ç”Ÿè¯" æˆ– "çŸ­è¯­" æˆ– "ç†Ÿè¯ç”Ÿä¹‰",
                "definition": "ä¸­æ–‡é‡Šä¹‰(å¿…é¡»å¯¹åº”æ–‡ä¸­çš„å…·ä½“å«ä¹‰)",
                "context": "åŒ…å«è¯¥è¯çš„å®Œæ•´åŸå¥"
            }}
        ]
        """
        user_prompt = f"ã€å¾…åˆ†ææ–‡æœ¬ã€‘(å‰5000å­—ç¬¦):\n{text[:5000]}"

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                response_format={ "type": "json_object" }
            )
            content = response.choices[0].message.content
            if "```" in content:
                content = re.search(r'\[.*\]', content, re.DOTALL).group()
            
            data = json.loads(content)
            if isinstance(data, dict):
                for key in data:
                    if isinstance(data[key], list): return data[key]
            return data
        except Exception as e:
            st.error(f"AI æ€è€ƒå‡ºé”™: {e}")
            return []

# ================= ä¸»é¡µé¢ UI =================

def main_tool_page():
    # 1. ä¾§è¾¹æ ï¼šè¿™é‡Œå±•ç¤ºä½ çš„è”ç³»æ–¹å¼
    st.sidebar.header("å…³äºé¡¹ç›®")
    st.sidebar.info("ğŸ“¢ å†…å®¹ä¸ºæŒç»­æ›´æ–°ä¸­\n\nğŸ’¬ å¾®ä¿¡: **lifeaka7**")
    
    # 2. æ£€æŸ¥æ˜¯å¦å®Œæˆæµ‹è¯•
    if 'user_vocab_size' not in st.session_state:
        run_vocab_test()
        return

    st.set_page_config(page_title="AI è‹±è¯­ç§æ•™", layout="wide")
    st.title("ğŸ§  AI è‹±è¯­ç§æ•™ (å¥½å‹å†…æµ‹ç‰ˆ)")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        st.success(f"ğŸ‘¤ ç”¨æˆ·æ¡£æ¡ˆ: **{st.session_state['user_vocab_size']} è¯**")
    with col2:
        if st.button("ğŸ”„ é‡æµ‹æ°´å¹³"):
            del st.session_state['user_vocab_size']
            st.session_state['test_stage'] = 1
            st.rerun()

    # ä½¿ç”¨éšè—çš„ Key
    miner = SmartMiner(MY_HIDDEN_KEY)

    tab1, tab2 = st.tabs(["ğŸ“„ æ–‡æœ¬æŒ–æ˜", "ğŸ“º YouTube æŒ–æ˜"])
    
    raw_text = ""
    with tab1:
        txt = st.text_area("ç²˜è´´è‹±æ–‡å†…å®¹:", height=150, placeholder="ç²˜è´´æ–‡ç« ...")
        if st.button("å¼€å§‹åˆ†æ (æ–‡æœ¬)"): raw_text = txt
            
    with tab2:
        if YOUTUBE_AVAILABLE:
            url = st.text_input("è§†é¢‘é“¾æ¥:")
            if st.button("å¼€å§‹åˆ†æ (è§†é¢‘)"): 
                raw = miner.get_youtube_text(url)
                if "Error" not in raw: raw_text = raw
                else: st.error(raw)
        else:
            st.warning("YouTube ç»„ä»¶ä¸å¯ç”¨")

    if raw_text:
        if len(raw_text) < 10:
            st.warning("å†…å®¹å¤ªçŸ­äº†")
        else:
            with st.spinner("ğŸ§  AI æ­£åœ¨åˆ†æ..."):
                results = miner.analyze_text_with_ai(raw_text, st.session_state['user_profile_prompt'])
            
            if results:
                st.balloons()
                st.write(f"### ğŸ¯ æŒ–æ˜ç»“æœ ({len(results)} ä¸ª)")
                df = pd.DataFrame(results)
                
                st.dataframe(
                    df, 
                    column_config={"word": "è¯æ±‡", "type": "ç±»å‹", "definition": "é‡Šä¹‰", "context": "åŸå¥"},
                    use_container_width=True
                )
                
                # Anki æ ¼å¼
                anki_df = pd.DataFrame()
                anki_df['Front'] = df.apply(lambda x: f"<b>{x['word']}</b> <small style='color:grey'>[{x['type']}]</small><br><br>{x['context']}", axis=1)
                anki_df['Back'] = df['definition']
                
                csv = anki_df.to_csv(index=False, header=False).encode('utf-8')
                st.download_button("ğŸ“¥ ä¸‹è½½ Anki æ–‡ä»¶ (.csv)", csv, "anki_cards.csv", "text/csv")

if __name__ == "__main__":
    main_tool_page()